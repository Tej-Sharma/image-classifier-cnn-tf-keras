{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Object Classifier CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA9VMTBJ8pSa",
        "colab_type": "text"
      },
      "source": [
        "#Object Classifier CNN\n",
        "This simple convolutional neural network will handle recognizing various common objects found in the world. Particularly, the following:\n",
        "  * Airplane\n",
        "  * Automobile\n",
        "  * Bird\n",
        "  * Cat\n",
        "  * Deer\n",
        "  * Dog\n",
        "  * Frog\n",
        "  * Horse\n",
        "  * Ship\n",
        "  * Truck "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neXhKor_8aad",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8f6ad90-cb6e-4237-99d2-c05bd409c0c3"
      },
      "source": [
        "# Get required imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AUXQZRH9QBS",
        "colab_type": "text"
      },
      "source": [
        "## Loading and setting up the data\n",
        "There is a dataset that comes with the Keras library that would suit our purposes very well here that includes the following class names.\n",
        "\n",
        "Data processing for this dataset is very minimal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zNPflu79nc3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8d299b77-705c-4577-b0db-c1b76fa7300c"
      },
      "source": [
        "#  Extract the trainining and test data\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "# Convert the grayscale images from 0-255 to 0-1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "CLASS_NAMES = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 12s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rl0U6rB91Hn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "ad155def-22f5-4c01-e37d-6eb1fce16d11"
      },
      "source": [
        "# Examine an image for reference\n",
        "plt.imshow(train_images[1], cmap=plt.cm.binary)\n",
        "plt.xlabel(CLASS_NAMES[train_labels[1][0]]) # It's a truck!\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIlf9M6Rk75Z",
        "colab_type": "text"
      },
      "source": [
        "## Creating the CNN Model\n",
        "We will add a stack of Conv2D and MaxPooling2D layers first.\n",
        "These will then be flattened and connected to dense layers to extract the overall object type from the image.\n",
        "\n",
        "First, the Conv layers will filter through the image to look for any potential patterns.\n",
        "\n",
        "Then, this will be flattened and passed onto dense layers to overall piece together what object the image represents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g_MukBulR4l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "d914464f-725c-4a47-a24f-f407404790d9"
      },
      "source": [
        "# Create the CNN model\n",
        "model = models.Sequential()\n",
        "# Input of 32 by 32 images with a depth of 3 with 32 filters of 3x3\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "# Pool with a stride of 2 and a size of 2 x 2\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "# Double the filters as there is a smaller image now, so more computation available\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "# Now, add the flattening and the dense layers\n",
        "# Note: with flattening the output shape is increased to 1024 outputs (32*32 pixels)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "# The output dense layer with a output size of the number of classes (10 here)\n",
        "model.add(layers.Dense(len(CLASS_NAMES)))\n",
        "model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                65600     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 122,570\n",
            "Trainable params: 122,570\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN6IHZIpmgOH",
        "colab_type": "text"
      },
      "source": [
        "## Model training\n",
        "We will be using the *adam* optimizer (for this simple neural network, it's perforamnce and memory efficiency are very good for the results it provides).\n",
        "\n",
        "Similarly, the *SparseCategoricalCrossentropy* is also a computationally efficient algorithm and uses up less memory as opposed to its vector-based variants.\n",
        "\n",
        "Currently, 4 epochs will be used for the model to find the right balance between underfitting and overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTkK-nums6sB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "4a501399-146a-4a88-92cf-1a4d8e310d34"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_images, train_labels, epochs=4, \n",
        "                    validation_data=(test_images, test_labels))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "  34/1563 [..............................] - ETA: 59s - loss: 2.2749 - accuracy: 0.1241 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-9fb30562dcb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m history = model.fit(train_images, train_labels, epochs=4, \n\u001b[0;32m----> 6\u001b[0;31m                     validation_data=(test_images, test_labels))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fwJZInZC5kS",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation\n",
        "Looking at the neural network, it can be seen that we achieve an accuracy of around 67 to 69%. This can be expected at this stage of the development. Thus, in order to further optimize the test accuracy, we are going to employ some optimizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O99Zj9PJwDp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print(\"Accuracy on test set: \", test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34ok5ZIcC37f",
        "colab_type": "text"
      },
      "source": [
        "## Improving the algorithm\n",
        "\n",
        "Currently only 50,000 examples are used. Image data augmentation can be used through Keras's ImageDataGenerator library to drastically increase the dataset.\n",
        "Such a small dataset is the main problem for the low accuracy. However, by altering the image (rotation, flip, translation, etc.), much more data can be generated."
      ]
    }
  ]
}